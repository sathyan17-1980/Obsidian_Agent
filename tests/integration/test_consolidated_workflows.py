"""Integration tests for multi-tool consolidated workflows."""

from pathlib import Path

import frontmatter  # type: ignore
import pytest

from src.agent.schemas import AgentDependencies
from src.shared.config import Settings
from src.tools.obsidian_graph_analyzer.service import analyze_graph_service
from src.tools.obsidian_note_manager.schemas import ManageNoteRequest, NoteOperation
from src.tools.obsidian_note_manager.service import manage_note_service
from src.tools.obsidian_vault_organizer.schemas import OrganizeOperation, OrganizeRequest
from src.tools.obsidian_vault_organizer.service import organize_vault_service
from src.tools.obsidian_vault_query.schemas import QueryMode, QueryRequest
from src.tools.obsidian_vault_query.service import query_vault_service


@pytest.mark.integration
@pytest.mark.asyncio
async def test_query_read_update_workflow(temp_vault: Path) -> None:
    """Test workflow: search for notes → read them → update based on findings.

    This simulates a common agent workflow:
    1. Query vault for notes matching criteria
    2. Read the found notes to analyze content
    3. Update a summary note with findings
    """
    # Setup: Create test notes with #project tag
    projects_dir = temp_vault / "projects"
    projects_dir.mkdir(exist_ok=True)

    (projects_dir / "alpha.md").write_text("""---
tags: [project, active]
status: in-progress
---

# Project Alpha

This is project alpha. It's currently active.
""")

    (projects_dir / "beta.md").write_text("""---
tags: [project, completed]
status: done
---

# Project Beta

This project is complete.
""")

    # Step 1: Query for notes tagged with #project
    query_request = QueryRequest(
        query="",
        mode=QueryMode.TAGS,
        tag_filters=["project"],
        max_results=10,
        response_format="minimal",
    )

    query_response = await query_vault_service(query_request, str(temp_vault))

    assert query_response.total_found > 0
    assert not query_response.truncated
    assert len(query_response.results) > 0

    # Step 2: Read the first found note with concise format
    first_note_path = query_response.results[0].path

    read_request = ManageNoteRequest(
        path=first_note_path,
        operation=NoteOperation.READ,
        response_format="concise",
    )

    read_response = await manage_note_service(
        read_request, str(temp_vault), max_file_size_mb=10
    )

    assert read_response.success
    assert "title" in read_response.data
    assert (
        "content" in read_response.data
        or "preview" in read_response.data
        or "content_preview" in read_response.data
    )

    # Step 3: Create a summary note with findings
    summary_content = f"""# Project Summary

Found {query_response.total_found} project notes.

## First Note Analysis
- **Title**: {read_response.data.get('title', 'Untitled')}
- **Tags**: {', '.join(read_response.data.get('tags', []))}
- **Path**: {first_note_path}

Generated by consolidated tools workflow test.
"""

    update_request = ManageNoteRequest(
        path="summaries/project-summary.md",
        operation=NoteOperation.UPDATE,
        content=summary_content,
        metadata_updates={"tags": ["summary", "automated"], "generated": "true"},
        response_format="minimal",
    )

    # Create summaries directory
    summaries_dir = temp_vault / "summaries"
    summaries_dir.mkdir(exist_ok=True)

    update_response = await manage_note_service(
        update_request, str(temp_vault), max_file_size_mb=10
    )

    assert update_response.success
    assert "summaries/project-summary.md" in update_response.message

    # Verify the summary was created
    verify_request = ManageNoteRequest(
        path="summaries/project-summary.md",
        operation=NoteOperation.READ,
        response_format="detailed",
    )

    verify_response = await manage_note_service(
        verify_request, str(temp_vault), max_file_size_mb=10
    )

    assert verify_response.success
    assert "Project Summary" in verify_response.data.get("content", "")
    assert "automated" in verify_response.data.get("tags", [])


@pytest.mark.integration
@pytest.mark.asyncio
async def test_graph_analysis_read_workflow(temp_vault: Path) -> None:
    """Test workflow: analyze knowledge graph → read linked notes.

    This simulates an agent exploring connections:
    1. Analyze graph to find linked notes
    2. Read the linked notes to understand relationships
    3. Update the center note with graph insights
    """
    from src.tools.obsidian_graph_analyzer.schemas import GraphAnalysisRequest

    # Setup: Create notes with wikilinks
    projects_dir = temp_vault / "projects"
    projects_dir.mkdir(exist_ok=True)

    (projects_dir / "alpha.md").write_text("""---
tags: [project]
---

# Project Alpha

Related to [[projects/beta]] and [[research/ai-safety]].
""")

    (projects_dir / "beta.md").write_text("""---
tags: [project]
---

# Project Beta

Works with [[projects/alpha]].
""")

    research_dir = temp_vault / "research"
    research_dir.mkdir(exist_ok=True)

    (research_dir / "ai-safety.md").write_text("""---
tags: [research]
---

# AI Safety Research

Referenced by multiple projects.
""")

    # Step 1: Analyze graph starting from a note with links
    graph_request = GraphAnalysisRequest(
        center_note="projects/alpha.md",
        depth=1,
        include_content_preview=True,
        response_format="concise",
    )

    graph_response = await analyze_graph_service(graph_request, str(temp_vault))

    assert graph_response.total_nodes > 0
    assert graph_response.depth_reached >= 1

    # Find nodes that are linked from center
    center_node = next(
        (node for node in graph_response.nodes if node.path == "projects/alpha.md"),
        None,
    )
    assert center_node is not None
    assert len(center_node.outbound_links) > 0 or len(center_node.inbound_links) > 0

    # Step 2: Read one of the linked notes
    linked_paths = center_node.outbound_links or center_node.inbound_links
    if linked_paths:
        linked_note_path = linked_paths[0]

        read_request = ManageNoteRequest(
            path=linked_note_path,
            operation=NoteOperation.READ,
            response_format="concise",
        )

        read_response = await manage_note_service(
            read_request, str(temp_vault), max_file_size_mb=10
        )

        assert read_response.success

    # Step 3: Update center note with graph insights
    graph_insights = f"""

---

## Graph Analysis

This note is connected to **{len(center_node.outbound_links)}** outbound links and **{len(center_node.inbound_links)}** inbound links.

**Total network size**: {graph_response.total_nodes} notes (depth 1)

**Linked notes**:
{chr(10).join(f'- [[{link}]]' for link in center_node.outbound_links[:5])}

*Analysis generated by graph analyzer workflow test.*
"""

    append_request = ManageNoteRequest(
        path="projects/alpha.md",
        operation=NoteOperation.APPEND,
        content=graph_insights,
        response_format="minimal",
    )

    append_response = await manage_note_service(
        append_request, str(temp_vault), max_file_size_mb=10
    )

    assert append_response.success

    # Verify the graph insights were added
    verify_request = ManageNoteRequest(
        path="projects/alpha.md",
        operation=NoteOperation.READ,
        response_format="detailed",
    )

    verify_response = await manage_note_service(
        verify_request, str(temp_vault), max_file_size_mb=10
    )

    assert verify_response.success
    assert "Graph Analysis" in verify_response.data.get("content", "")
    assert "Total network size" in verify_response.data.get("content", "")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_query_organize_tag_workflow(temp_vault: Path) -> None:
    """Test workflow: search for notes → batch tag them.

    This simulates organizing notes based on search results:
    1. Query vault for notes matching criteria
    2. Batch tag all found notes for organization
    3. Verify tags were applied
    """
    # Setup: Create notes to organize
    drafts_dir = temp_vault / "drafts"
    drafts_dir.mkdir(exist_ok=True)

    for i in range(3):
        note = drafts_dir / f"draft-{i}.md"
        post = frontmatter.Post(f"# Draft {i}\n\nContent here.", tags=["draft"])
        note.write_text(frontmatter.dumps(post))

    # Step 1: Query for all draft notes
    query_request = QueryRequest(
        query="",
        mode=QueryMode.TAGS,
        tag_filters=["draft"],
        max_results=10,
        response_format="minimal",
    )

    query_response = await query_vault_service(query_request, str(temp_vault))

    assert query_response.total_found == 3
    note_paths = [result.path for result in query_response.results]

    # Step 2: Batch tag all found notes as "reviewed"
    organize_request = OrganizeRequest(
        notes=note_paths,
        operation=OrganizeOperation.TAG,
        tags_to_add=["reviewed", "ready-for-publish"],
        tags_to_remove=["draft"],
    )

    organize_response = await organize_vault_service(organize_request, str(temp_vault))

    assert organize_response.total_succeeded == 3
    assert organize_response.total_failed == 0

    # Step 3: Verify tags were applied by re-querying
    verify_query = QueryRequest(
        query="",
        mode=QueryMode.TAGS,
        tag_filters=["reviewed"],
        max_results=10,
        response_format="minimal",
    )

    verify_response = await query_vault_service(verify_query, str(temp_vault))

    assert verify_response.total_found == 3

    # Verify draft tag was removed
    draft_query = QueryRequest(
        query="",
        mode=QueryMode.TAGS,
        tag_filters=["draft"],
        max_results=10,
        response_format="minimal",
    )

    draft_response = await query_vault_service(draft_query, str(temp_vault))

    assert draft_response.total_found == 0  # All draft tags removed


@pytest.mark.integration
@pytest.mark.asyncio
async def test_query_organize_archive_workflow(temp_vault: Path) -> None:
    """Test workflow: search for old notes → archive them.

    This simulates cleanup workflows:
    1. Query for notes to archive (e.g., completed projects)
    2. Batch archive them with timestamp
    3. Verify they moved to archive
    """
    # Setup: Create completed project notes
    projects_dir = temp_vault / "projects"
    projects_dir.mkdir(exist_ok=True)

    completed_notes = []
    for i in range(2):
        note = projects_dir / f"old-project-{i}.md"
        post = frontmatter.Post(
            f"# Old Project {i}\n\nCompleted.", tags=["project"], status="completed"
        )
        note.write_text(frontmatter.dumps(post))
        completed_notes.append(f"projects/old-project-{i}.md")

    # Step 1: Query for completed projects
    query_request = QueryRequest(
        query="",
        mode=QueryMode.PROPERTIES,
        property_filters={"status": "completed"},
        max_results=10,
        response_format="minimal",
    )

    query_response = await query_vault_service(query_request, str(temp_vault))

    assert query_response.total_found == 2

    # Step 2: Archive found notes (auto-generates timestamp path)
    organize_request = OrganizeRequest(
        notes=completed_notes,
        operation=OrganizeOperation.ARCHIVE,
        # destination=None → auto-generates archive/YYYY-MM-DD/
    )

    organize_response = await organize_vault_service(organize_request, str(temp_vault))

    assert organize_response.total_succeeded == 2
    assert organize_response.total_failed == 0

    # Step 3: Verify notes moved to archive
    archive_dir = temp_vault / "archive"
    assert archive_dir.exists()

    # Check that dated subdirectory was created
    dated_dirs = list(archive_dir.iterdir())
    assert len(dated_dirs) == 1
    assert (dated_dirs[0] / "old-project-0.md").exists()
    assert (dated_dirs[0] / "old-project-1.md").exists()

    # Verify original location is empty
    assert not (projects_dir / "old-project-0.md").exists()
    assert not (projects_dir / "old-project-1.md").exists()


@pytest.mark.integration
@pytest.mark.asyncio
async def test_query_read_organize_move_workflow(temp_vault: Path) -> None:
    """Test workflow: search → read to validate → move to organized location.

    This simulates intelligent organization:
    1. Query for notes to reorganize
    2. Read them to extract metadata
    3. Move them to appropriate folders based on content
    """
    # Setup: Create notes in inbox that need organizing
    inbox_dir = temp_vault / "inbox"
    inbox_dir.mkdir(exist_ok=True)

    (inbox_dir / "meeting-notes.md").write_text(
        """---
tags: [meeting]
type: meeting
---

# Meeting Notes

Discussion about Q1 planning.
"""
    )

    (inbox_dir / "idea.md").write_text(
        """---
tags: [idea]
type: idea
---

# Random Idea

New feature concept.
"""
    )

    # Step 1: Query inbox for notes
    query_request = QueryRequest(
        query="",
        mode=QueryMode.FULLTEXT,
        search_term="",
        path_filter="inbox",
        max_results=10,
        response_format="concise",
    )

    query_response = await query_vault_service(query_request, str(temp_vault))

    assert query_response.total_found == 2

    # Step 2: Read each note to determine destination
    notes_to_organize = []

    for result in query_response.results:
        read_request = ManageNoteRequest(
            path=result.path,
            operation=NoteOperation.READ,
            response_format="concise",
        )

        read_response = await manage_note_service(
            read_request, str(temp_vault), max_file_size_mb=10
        )

        # In concise format, frontmatter fields are nested under "frontmatter"
        note_type = read_response.data.get("frontmatter", {}).get("type", "unknown")

        # Determine destination based on type
        if note_type == "meeting":
            destination = "meetings"
        elif note_type == "idea":
            destination = "ideas"
        else:
            destination = "misc"

        notes_to_organize.append((result.path, destination))

    # Step 3: Move meeting note
    meeting_notes = [path for path, dest in notes_to_organize if dest == "meetings"]

    if meeting_notes:
        # Create destination directory
        (temp_vault / "meetings").mkdir(exist_ok=True)

        organize_request = OrganizeRequest(
            notes=meeting_notes,
            operation=OrganizeOperation.MOVE,
            destination="meetings",
        )

        organize_response = await organize_vault_service(
            organize_request, str(temp_vault)
        )

        assert organize_response.total_succeeded == 1
        assert (temp_vault / "meetings" / "meeting-notes.md").exists()

    # Step 4: Move idea note
    idea_notes = [path for path, dest in notes_to_organize if dest == "ideas"]

    if idea_notes:
        # Create destination directory
        (temp_vault / "ideas").mkdir(exist_ok=True)

        organize_request = OrganizeRequest(
            notes=idea_notes,
            operation=OrganizeOperation.MOVE,
            destination="ideas",
        )

        organize_response = await organize_vault_service(
            organize_request, str(temp_vault)
        )

        assert organize_response.total_succeeded == 1
        assert (temp_vault / "ideas" / "idea.md").exists()

    # Verify inbox is empty
    remaining_in_inbox = list(inbox_dir.glob("*.md"))
    assert len(remaining_in_inbox) == 0


@pytest.mark.integration
@pytest.mark.asyncio
async def test_graph_analyze_tag_connections_workflow(temp_vault: Path) -> None:
    """Test workflow: analyze graph → tag notes based on connections.

    This simulates intelligent tagging based on relationships:
    1. Analyze knowledge graph to find hub notes
    2. Tag highly connected notes as "hub"
    3. Tag isolated notes as "orphan"
    """
    from src.tools.obsidian_graph_analyzer.schemas import GraphAnalysisRequest

    # Setup: Create network of notes
    (temp_vault / "central.md").write_text(
        """---
tags: [central]
---

# Central Hub

Links to [[peripheral-1]], [[peripheral-2]], [[peripheral-3]].
"""
    )

    for i in range(3):
        (temp_vault / f"peripheral-{i+1}.md").write_text(
            f"""---
tags: [peripheral]
---

# Peripheral {i+1}

Related to [[central]].
"""
        )

    (temp_vault / "isolated.md").write_text(
        """---
tags: [isolated]
---

# Isolated Note

No links to other notes.
"""
    )

    # Step 1: Analyze graph from central note
    graph_request = GraphAnalysisRequest(
        center_note="central.md",
        depth=1,
        include_content_preview=False,
        response_format="minimal",
    )

    graph_response = await analyze_graph_service(graph_request, str(temp_vault))

    # Find central node
    central_node = next(
        (node for node in graph_response.nodes if node.path == "central.md"),
        None,
    )

    assert central_node is not None

    # Step 2: Tag central note as "hub" (has many outbound links)
    if len(central_node.outbound_links) >= 3:
        organize_request = OrganizeRequest(
            notes=["central.md"],
            operation=OrganizeOperation.TAG,
            tags_to_add=["hub", "well-connected"],
        )

        organize_response = await organize_vault_service(
            organize_request, str(temp_vault)
        )

        assert organize_response.total_succeeded == 1

    # Step 3: Analyze isolated note
    isolated_graph_request = GraphAnalysisRequest(
        center_note="isolated.md",
        depth=1,
        include_content_preview=False,
        response_format="minimal",
    )

    isolated_graph_response = await analyze_graph_service(
        isolated_graph_request, str(temp_vault)
    )

    isolated_node = next(
        (node for node in isolated_graph_response.nodes if node.path == "isolated.md"),
        None,
    )

    # Step 4: Tag isolated notes as "orphan"
    if (
        isolated_node
        and len(isolated_node.outbound_links) == 0
        and len(isolated_node.inbound_links) == 0
    ):
        organize_request = OrganizeRequest(
            notes=["isolated.md"],
            operation=OrganizeOperation.TAG,
            tags_to_add=["orphan", "needs-linking"],
        )

        organize_response = await organize_vault_service(
            organize_request, str(temp_vault)
        )

        assert organize_response.total_succeeded == 1

    # Step 5: Verify tags were applied
    verify_central = ManageNoteRequest(
        path="central.md",
        operation=NoteOperation.READ,
        response_format="concise",
    )

    central_response = await manage_note_service(
        verify_central, str(temp_vault), max_file_size_mb=10
    )

    assert "hub" in central_response.data.get("tags", [])

    verify_isolated = ManageNoteRequest(
        path="isolated.md",
        operation=NoteOperation.READ,
        response_format="concise",
    )

    isolated_response = await manage_note_service(
        verify_isolated, str(temp_vault), max_file_size_mb=10
    )

    assert "orphan" in isolated_response.data.get("tags", [])
